package com.misterjerry.test01.ui

import android.app.Application
import android.content.Intent
import android.os.Bundle
import android.speech.RecognitionListener
import android.speech.RecognizerIntent
import android.speech.SpeechRecognizer
import androidx.lifecycle.AndroidViewModel
import androidx.lifecycle.viewModelScope
import com.misterjerry.test01.data.AudioClassifierHelper
import com.misterjerry.test01.data.ConversationItem
import com.misterjerry.test01.data.SoundEvent
import com.misterjerry.test01.data.SoundRepository
import com.misterjerry.test01.data.Urgency
import com.misterjerry.test01.util.VibrationHelper
import kotlinx.coroutines.Dispatchers
import kotlinx.coroutines.flow.MutableStateFlow
import kotlinx.coroutines.flow.SharingStarted
import kotlinx.coroutines.flow.StateFlow
import kotlinx.coroutines.flow.combine
import kotlinx.coroutines.flow.stateIn
import kotlinx.coroutines.launch
import com.misterjerry.test01.data.SoundSettings
import com.misterjerry.test01.data.VibrationPattern

data class MainUiState(
    val soundEvents: List<SoundEvent> = emptyList(),
    val conversationHistory: List<ConversationItem> = emptyList(),
    val isListening: Boolean = false,
    val soundSettings: SoundSettings = SoundSettings()
)


class MainViewModel(application: Application) : AndroidViewModel(application) {
    private val soundRepository = SoundRepository()
    // Remove ConversationRepository as we will generate real data
    // private val conversationRepository = ConversationRepository()

    private val _conversationHistory = MutableStateFlow<List<ConversationItem>>(emptyList())
    private val _isListening = MutableStateFlow(false)
    private val _soundSettings = MutableStateFlow(SoundSettings())

    private val speechRecognizer: SpeechRecognizer = SpeechRecognizer.createSpeechRecognizer(application)
    private val audioClassifierHelper = AudioClassifierHelper(application)
    private val vibrationHelper = VibrationHelper(application)

    init {
        // ... (SpeechRecognizer init code remains same) ...
        
        // Listen to audio classification results
        viewModelScope.launch {
            audioClassifierHelper.classificationFlow.collect { (label, direction) ->
                handleSoundClassification(label, direction)
            }
        }
    }

    // ... (SpeechRecognizer methods remain same) ...

    fun startEnvironmentMode() {
        audioClassifierHelper.startAudioClassification()
    }

    fun stopEnvironmentMode() {
        audioClassifierHelper.stopAudioClassification()
    }

    private fun handleSoundClassification(label: String, direction: Float) {
        val (koreanLabel, urgency) = when (label) {
            // Safety (High Urgency)
            "Siren", "Ambulance (siren)", "Fire engine, fire truck (siren)" -> "ì‚¬ì´ë Œ" to Urgency.HIGH
            "Car horn, honking" -> "ìžë™ì°¨ ê²½ì " to Urgency.HIGH
            "Baby cry, infant cry" -> "ì•„ê¸° ìš¸ìŒì†Œë¦¬" to Urgency.HIGH
            "Smoke detector, smoke alarm" -> "í™”ìž¬ ê²½ë³´ê¸°" to Urgency.HIGH
            "Glass" -> "ìœ ë¦¬ ê¹¨ì§€ëŠ” ì†Œë¦¬" to Urgency.HIGH
            "Scream" -> "ë¹„ëª… ì†Œë¦¬" to Urgency.HIGH

            // Alerts / Communication (Medium Urgency)
            "Doorbell" -> "ì´ˆì¸ì¢… ì†Œë¦¬" to Urgency.MEDIUM
            "Telephone", "Ringtone" -> "ì „í™” ë²¨ì†Œë¦¬" to Urgency.MEDIUM
            "Alarm" -> "ì•ŒëžŒ ì†Œë¦¬" to Urgency.MEDIUM
            "Dog", "Bark" -> "ê°œ ì§–ëŠ” ì†Œë¦¬" to Urgency.MEDIUM

            // Daily Life (Low Urgency)
            "Clapping", "Hands" -> "ë°•ìˆ˜ ì†Œë¦¬" to Urgency.LOW
            "Knock" -> "ë…¸í¬ ì†Œë¦¬" to Urgency.LOW
            "Finger snapping" -> "í•‘ê±° ìŠ¤ëƒ…" to Urgency.LOW
            "Speech" -> "ë§ì†Œë¦¬" to Urgency.LOW
            "Water tap, faucet" -> "ë¬¼ í‹€ì–´ë†“ì€ ì†Œë¦¬" to Urgency.LOW
            "Toilet flush" -> "ë³€ê¸° ë¬¼ ë‚´ë¦¬ëŠ” ì†Œë¦¬" to Urgency.LOW
            "Microwave oven" -> "ì „ìžë ˆì¸ì§€ ì†Œë¦¬" to Urgency.LOW
            "Cat", "Meow" -> "ê³ ì–‘ì´ ìš¸ìŒì†Œë¦¬" to Urgency.LOW

            else -> return // Ignore other sounds for now
        }

        val newEvent = SoundEvent(
            id = System.currentTimeMillis(),
            name = koreanLabel,
            direction = direction,
            distance = (1..10).random().toFloat(), // Random distance for demo
            urgency = urgency
        )

        val settings = _soundSettings.value
        val urgencySetting = when (urgency) {
            Urgency.HIGH -> settings.highUrgency
            Urgency.MEDIUM -> settings.mediumUrgency
            Urgency.LOW -> settings.lowUrgency
        }

        if (urgencySetting.isEnabled) {
            vibrationHelper.vibrate(urgencySetting.vibrationPattern)

            // Update sound events list (keep events within last 1 hour)
            val currentEvents = uiState.value.soundEvents
            val oneHourAgo = System.currentTimeMillis() - 3600000 // 1 hour in millis
            val updatedEvents = (listOf(newEvent) + currentEvents).filter { it.id > oneHourAgo }

            _soundEventsFlow.value = updatedEvents
        } else {
            // Update sound events list (just filter old events)
            val currentEvents = uiState.value.soundEvents
            val oneHourAgo = System.currentTimeMillis() - 3600000 // 1 hour in millis
            val updatedEvents = currentEvents.filter { it.id > oneHourAgo }

            _soundEventsFlow.value = updatedEvents
        }
    }

    // We need to replace the repository flow with a local flow
    private val _soundEventsFlow = MutableStateFlow<List<SoundEvent>>(emptyList())

    private val recognitionIntent = Intent(RecognizerIntent.ACTION_RECOGNIZE_SPEECH).apply {
        putExtra(RecognizerIntent.EXTRA_LANGUAGE_MODEL, RecognizerIntent.LANGUAGE_MODEL_FREE_FORM)
        putExtra(RecognizerIntent.EXTRA_LANGUAGE, "ko-KR")
        putExtra(RecognizerIntent.EXTRA_PARTIAL_RESULTS, true)
    }

    init {
        speechRecognizer.setRecognitionListener(object : RecognitionListener {
            override fun onReadyForSpeech(params: Bundle?) {}
            override fun onBeginningOfSpeech() {}
            override fun onRmsChanged(rmsdB: Float) {}
            override fun onBufferReceived(buffer: ByteArray?) {}
            override fun onEndOfSpeech() {
                _isListening.value = false
            }

            override fun onError(error: Int) {
                _isListening.value = false
                // Handle error if needed
            }

            override fun onResults(results: Bundle?) {
                val matches = results?.getStringArrayList(SpeechRecognizer.RESULTS_RECOGNITION)
                if (!matches.isNullOrEmpty()) {
                    val text = matches[0]
                    addConversationItem(text)
                }
            }

            override fun onPartialResults(partialResults: Bundle?) {}
            override fun onEvent(eventType: Int, params: Bundle?) {}
        })
    }

    val uiState: StateFlow<MainUiState> = combine(
        _soundEventsFlow,
        _conversationHistory,
        _isListening,
        _soundSettings,
    ) { sounds, history, isListening, settings ->
        MainUiState(
            soundEvents = sounds,
            conversationHistory = history,
            isListening = isListening,
            soundSettings = settings
        )
    }.stateIn(
        scope = viewModelScope,
        started = SharingStarted.WhileSubscribed(5000),
        initialValue = MainUiState()
    )

    fun startListening() {
        viewModelScope.launch(Dispatchers.Main) {
            _isListening.value = true
            speechRecognizer.startListening(recognitionIntent)
        }
    }

    fun stopListening() {
        viewModelScope.launch(Dispatchers.Main) {
            _isListening.value = false
            speechRecognizer.stopListening()
        }
    }

    private fun addConversationItem(text: String) {
        viewModelScope.launch {
            // 1. Add item immediately with loading state
            val tempId = System.currentTimeMillis()
            val tempItem = ConversationItem(
                id = tempId,
                speaker = "ìƒëŒ€ë°©",
                text = text,
                emotion = "",
                emotionLabel = "",
                isUser = false,
                timestamp = java.text.SimpleDateFormat("a h:mm", java.util.Locale.KOREA).format(java.util.Date()),
                isLoading = true
            )
            
            _conversationHistory.value = _conversationHistory.value + tempItem

            // 2. Analyze emotion
            val emotionLabel = analyzeEmotionWithGpt(text)
            val emotionEmoji = when (emotionLabel) {
                "ê¸ì •" -> "ðŸ˜ƒ"
                "ë¶€ì •" -> "ðŸ˜ "
                "ë†€ëžŒ" -> "ðŸ˜²"
                "ìŠ¬í””" -> "ðŸ˜¢"
                "ê³µí¬" -> "ðŸ˜¨"
                "ê±±ì •" -> "ðŸ˜Ÿ"
                else -> "ðŸ˜"
            }

            // 3. Update item with result
            val updatedList = _conversationHistory.value.map { item ->
                if (item.id == tempId) {
                    item.copy(
                        emotion = emotionEmoji,
                        emotionLabel = emotionLabel,
                        isLoading = false
                    )
                } else {
                    item
                }
            }
            _conversationHistory.value = updatedList
        }
    }

    private suspend fun analyzeEmotionWithGpt(text: String): String {
        return try {
            val prompt = "ë‹¤ìŒ í…ìŠ¤íŠ¸ì˜ ê°ì •ì„ ë¶„ì„í•´ì„œ 'ê¸ì •', 'ë¶€ì •', 'ì¤‘ë¦½', 'ë†€ëžŒ', 'ìŠ¬í””', 'ê³µí¬', 'ê±±ì •' ì¤‘ í•˜ë‚˜ë¡œë§Œ ëŒ€ë‹µí•´ì¤˜. í…ìŠ¤íŠ¸: $text"
            val request = com.misterjerry.test01.data.api.ChatRequest(
                messages = listOf(
                    com.misterjerry.test01.data.api.Message(role = "user", content = prompt)
                )
            )
            val response = com.misterjerry.test01.data.api.RetrofitClient.instance.getChatCompletion(request)
            val content = response.choices.firstOrNull()?.message?.content?.trim() ?: "ì¤‘ë¦½"
            
            // Validate response just in case
            if (content in listOf("ê¸ì •", "ë¶€ì •", "ì¤‘ë¦½", "ë†€ëžŒ", "ìŠ¬í””", "ê³µí¬", "ê±±ì •")) content else "ì¤‘ë¦½"
        } catch (e: Exception) {
            e.printStackTrace()
            // Fallback to heuristic analysis
            analyzeEmotion(text)
        }
    }

    private fun analyzeEmotion(text: String): String {
        return when {
            text.contains("í™”ë‚˜") || text.contains("ì§œì¦") -> "ë¶€ì •"
            text.contains("í–‰ë³µ") || text.contains("ì¢‹ì•„") || text.contains("ì‚¬ëž‘") -> "ê¸ì •"
            text.contains("ë†€ë¼") || text.contains("í—‰") -> "ë†€ëžŒ"
            text.contains("ìŠ¬í¼") || text.contains("ìš°ìš¸") -> "ìŠ¬í””"
            text.contains("ë¬´ì„œ") || text.contains("ê³µí¬") -> "ê³µí¬"
            text.contains("ê±±ì •") || text.contains("ë¶ˆì•ˆ") || text.contains("ê·¼ì‹¬") -> "ê±±ì •"
            else -> "ì¤‘ë¦½"
        }
    }

    fun updateSoundSettings(newSettings: SoundSettings) {
        _soundSettings.value = newSettings
    }

    fun clearSoundEvents() {
        _soundEventsFlow.value = emptyList()
    }

    override fun onCleared() {
        super.onCleared()
        speechRecognizer.destroy()
    }
}
